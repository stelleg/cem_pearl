\documentclass[preprint]{sigplanconf}

\usepackage{inconsolata}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{mathtools}
\usetikzlibrary{chains,fit,shapes,calc}
\usepackage{semantic}
\usepackage{tabu}
\usepackage{amsthm}
\usepackage{mathptmx}
\usepackage{todonotes}
\usepackage{listings}
\lstset{
  frame=none,
  xleftmargin=6pt,
  stepnumber=1,
  belowcaptionskip=\bigskipamount,
  captionpos=b,
  escapeinside={*'}{'*},
  language=haskell,
  tabsize=2,
  emphstyle={\bf},
  commentstyle=\it\ttfamily,
  stringstyle=\mdseries\ttfamily\color{violet},
  showspaces=false,
  keywordstyle=\bfseries\ttfamily\color{teal},
  columns=flexible,
  basicstyle=\small\ttfamily,
  showstringspaces=false,
  morecomment=[l]\%
}
\lstnewenvironment{code}{}{}

\begin{document}
\title{Shared Environment Call-By-Need}
\subtitle{A Functional Pearl}

\authorinfo{George Stelle}
           {University of New Mexico}
           {stelleg@cs.unm.edu}
\authorinfo{Darko Stefanovic}
           {University of New Mexico}
           {darko@cs.unm.edu}
\authorinfo{Stephen L. Olivier}
           {Sandia National Laboratories}
           {slolivi@sandia.gov}
\authorinfo{Stephanie Forrest}
           {University of New Mexico}
           {forrest@cs.unm.edu}

\maketitle

\begin{abstract}
Existing implementations of lazy evaluation use a \emph{flat} representation
of environments, storing the locations of closures associated with free
variables in an array. Combined with a heap, this structure supports the
shared intermediate results required by lazy evaluation.  Here, we take an
alternative approach, compiling lambda terms to x64 assembly. By using
\emph{shared} environments to implement sharing of intermediate results, we
achieve an implementation simpler than any implementation of the call-by-need
lambda calculus of which we are aware. The semantics of this approach are
captured by a new abstract machine, based on lazy variants of Krivine's call by
name abstract machine. Beyond simplicity, our approach enables explorations of
the tradeoffs between flat and shared environment structures for lazy languages.
\end{abstract}

\section{Introduction} \label{sec:intro}

Lazy functional programming languages implement call-by-need lambda calculus
\cite{ariola1995call}. In these implementations, environments are \emph{flat},
binding free variables to locations in the heap and enforcing proper sharing.
Here, we propose an alternative approach, which implements the sharing required
for call-by-need semantics with a single shared environment, which serves both
as the environment for every closure and as the heap. 

In this pearl, we focus only on the issue of implementation simplicity, leaving
performance measurements for future work, although we believe that our approach
could also lead to efficiency improvements. This frees us to focus on lambda
calculus, and not worry about efficient implementation of machine literals and
primitive operations. 

The core of the paper is a novel lazy Krivine machine, seen in
Figure~\ref{fig:L}. Using this abstract machine, we present an x64 native code
compiler written in Haskell that is so simple that its essential components fit
on a single page, as shown in Figure~\ref{fig:impl}.\footnote{For brevity we
omit well-understood components---the parser, the translation to deBruijn
indices, and the x64 preamble; the full compiler is available at
\url{http://github.com/stelleg/cem_pearl}.} 

\begin{code}
compileProgram :: ClosedTerm -> [BasicBlock]
data BasicBlock = BB Label [Instr]
type Label = String
type Instr = String
\end{code}

The \texttt{compileProgram} function takes as input a closed lambda term and
produces as output a list of x64 basic blocks. Recall that a basic block
consists is a sequence of instructions with a single entry and exit point.
Since we use only a small subset of the available x64 instructions, a simple
string representation suffices. The rest of the paper builds the prerequisites
to understand this function.

\section{Preliminaries}
We assume familiarity with lambda calculus, environments, and closures.
We adopt standard Barendregt syntax \cite{barendregt1984lambda} for lambda
terms, $$ t ::=  \lambda x.t \; | \; t \;  t \; | \; x $$, applications are
left associative, and lambda bodies extend as far as possible. 

Internally the compiler operates on lambda terms with deBruijn indices, so we
add the syntax $$ t ::= \lambda t \; | \; t \; t \; | \; i $$ where $i \in
\mathbb{N}$ replaces the variable name with an index to its binding lambda. To
ensure that our program is closed, we first implement the standard unary natural
number type \texttt{N} and a type representing the set of natural numbers up to
\texttt{n}, \texttt{Fin n}. We then parameterize \texttt{Term} on an \texttt{N}
that constrains the maximum number of free variables. \texttt{Lam} increases
that index by one, while \texttt{Var} takes a parameter which can only index
into that depth, using our \texttt{Fin n} type.

\begin{code}
data N = Z | S N
data Fin n where
  FZ :: Fin (S k)
  FS :: Fin k -> Fin (S k)

data Term n where
  Lam :: Term (S n) -> Term n
  Var :: Fin n -> Term n 
  App :: Term n -> Term n -> Term n

type ClosedTerm = Term Z
\end{code}

Instead of binding named variables, here a variable indexes the depth of its
binding lambda with a natural number. For example $\lambda t.\lambda f.t$ in
standard notation becomes $\lambda\lambda1$ in lambda calculus with deBruijn
indices.  Examples are easier to write, read, and understand in standard lambda
calculus with named variables, and we will do so.

In mechanical evaluation of terms, it would be too inefficient to perform
explicit substitution. To solve this, the standard approach uses
closures~\cite{landin1964mechanical, curien1991abstract, jonesstg,
biernacka2007concrete}. Closures combine an term with an environment, binding
the free variables in the term to other closures. We follow this
approach, but with a novel twist. 

There are three standard approaches to evaluation of lambda expressions to weak
head normal form: call-by-value, call-by-name, and call-by-need. The three
approaches can be summarized as follows: call-by-value evaluates arguments
exactly once, call-by-name evaluates arguments term zero or more times, and
call-by-need evaluates arguments at most once. In this paper we are concerned
with implementing call-by-need. \footnote{There is some ambiguity in the
literature regarding the use of the term \emph{lazy}. We will use the term to
mean call-by-need.}

\section{Environment Representation} \label{sec:env}

There are two common approaches that span the design space for environment
representation: \emph{flat} and \emph{shared} (also
known as linked)~\cite{appel1988optimizing, shao1994space}. In a flat
environment, each closure has its own record of what closures all
of its free variables are bound to. In a shared environment, parts
of that record are shared among multiple environments~\cite{appel1988optimizing,
shao1994space}. For example, consider the following closed term: $$(\lambda
x.(\lambda y.t) (\lambda z.t_1)) t_2$$ Assuming the term $t$ has both $x$ and
$y$ as free variables, we must evaluate it in the environment binding both $x$
and $y$.  Similarly, assuming $t_1$ contains both $z$ and $x$ as free variables,
we must evaluate it in an environment containing bindings for both $x$ and $z$.
Thus, we can represent the closures for $t$ and $t_1$ as $$t[x=t_2[\bullet],
y=c]$$ and $$t_1[x=t_2[\bullet], z=c_1]$$ respectively. In a flat
representation, space is allocated separately for each environment, and each
closure contains an explicit record of all of its free variables. Because of the
nested scope of the given term, $x$ is bound to the identical closure in each
environment and can also be represented by a shared, linked structure, shown in
the following diagram:

\begin{center}
\begin{tikzpicture}[ 
  edge from parent path={(\tikzchildnode\tikzchildanchor) edge [-latex] (\tikzparentnode\tikzparentanchor)},
  level distance=1cm
]
\node (d) {$\bullet$} child{node (a) {$x=t_2[\bullet]$} child{node (b) {$y=c$}} child{node (c)
{$z=c_1$}}};

\end{tikzpicture}
\end{center}
In this scheme, each environment is represented by a linked list, with
the binding of $x$ shared between them. This simple example
illustrates a shared, linked environment~\cite{appel1988optimizing}
and dates back to Landin's SECD machine~\cite{landin1964mechanical}.

The strengths and weaknesses of each approach are well known.  With a flat
environment variable lookup is implemented by a simple offset, or even
bound to a particular register~\cite{jonesstg, appel2006compiling},
but significant duplication can occur.  In a shared environment, unnecessary
duplication is avoided, but a cost is incurred on dereference whenever the
link structure must be traversed.

\subsection{Call-By-Need and the Heap}

Historically, flat environments have dominated call-by-need implementations
\cite{jonesstg, TIM, johnsson1984efficient, boquist1997grin,
diehl2000abstract}. We speculate that this is in part because call-by-need
evaluation has historically been implemented with combinators, which take all
their free variables as formal parameters, and therefore suit flat environments
well. Note that for call-by-need languages, one must be careful when using flat
environments.  Because results from bound computations have to be shared across
closures referencing them, an implementation of call-by-need cannot copy
closures across environments. To illustrate this, consider again the earlier
example.  If we copied closures, results from evaluation of $t_2[\bullet]$ from
dereferencing $x$ in $t$ would not be shared with the instances of $x$ in
$t_1$.  This is fine for call-by-name, but to implement call by need, we must
ensure sharing.

The standard approach is to add a heap, which maps addresses to closures
\cite{jonesstg, TIM, johnsson1984efficient, sestoft} and then
modify the environments to map variables to addresses in the heap. The final
step adds update markers pointing to heap addresses on the stack, to update
the closures at those locations in the heap with their value once they have been
evaluated.  Reconsidering the shared environment diagram from the preceding
section, if we updated the $x=t_2[\bullet]$ with $t_2[\bullet]$'s corresponding
value after it was entered, the result would be shared between $t$ and $t_2$
without an additional heap. This is what we now describe and implement.

\section{Shared Environment Call-By-Need} \label{sec:calc}

Here we describe the key idea of our approach, showing how
the shared environment described earlier can be applied to call-by-need
evaluation. We start with a well-known abstract machine for evaluating
call-by-name, the Krivine machine $K$, Figure~\ref{fig:Krivine}.
The mechanics are simple: applications push their argument onto the stack,
abstractions pull an argument off the stack and move it into the environment,
and variables enter the closure at their index into the environment.

\input{figures/K}

Note that we have not specified how the environment is represented in the $K$
machine. To move towards the goal of sharing via environment structure,
we force the environment to be shared by creating a heap of environment
cells, and then extending each cell manually. This produces the $K'$ machine,
Figure~\ref{fig:K'}.

\input{figures/Kprime}

Now, we have a heap of environments, which ensures that the
environments are shared when extended.  The $K'$ machine is identical to the $K$
machine, except that it forces this sharing of environments. 

Our explicitly shared environments enable us to add the necessary
machinery to implement full call-by-need semantics. We modify the $K'$ machine
to make it lazy in much the same way as Sestoft modified the Krivine
machine~\cite{sestoft}. We add update markers so that when a closure is entered,
the Var1 rule pushes an update marker to the corresponding location onto the
stack.  Then, when the corresponding value is our current closure, it will pop
the update marker, and replace the closure on the heap with itself. This
call-by-need $L$ machine is specified in Figure~\ref{fig:L}.

\input{figures/L}

We see from the semantics that all that has changed between the $K'$ and the
$L$ machine is the addition of the update marker to the Stack 
, and the corresponding Update and Var1 rules. This is in contrast to flat
environment machines, such as the TIM~\cite{TIM}, which require more complex
machinery to ensure proper sharing.

Our $L$ machine is similar to existing lazy variants of the Krivine machine,
such as that defined by Sestoft \cite{sestoft}, except that instead of leaving
the environment structure unspecified, and adding a heap of closures, we have
forced the environment to be shared, and used that shared environment to
implement the necessary sharing of results.\footnote{In fact, in the code
comments of his dissertation, Sestoft mentions possible environment sharing, but
it is never formalized \cite{sestoft91analysis}.} 

\section{An Example}

We now turn to a visualization of an example execution to provide intuition for
how the $L$ machine operates. Figure~\ref{fig:states} shows a portion of the
execution trace during the evaluation of $$(\lambda a.(\lambda b.b \; a)
(\lambda c.c \; a)) ((\lambda i.i) (\lambda j.j))$$.  \input{figures/states} We
can see each rule in effect in our visualization, and how it changes the state
of the machine, directly. This should give the reader some intuition for how the
machine operates, and how simple its rules are from a mechanical point of view.
See the caption for a detailed description of state transitions. 

\section{Implementation}\label{sec:impl}

First, some preliminaries. We target the x64 GNU assembler running on
Linux. Any x64 Linux machine with GNU \texttt{as} and GNU \texttt{ld} should
be able to run the compiled examples.  We include in the data type a label for
each basic block so it can be jumped to directly, and we annotate each x64
instruction line with a comment summarizing its purpose.  We also explain the
simple mapping between each basic block and the corresponding rule from the
abstract machine specification.

The \texttt{compile} function takes a deBruijn indexed lambda term and
generates a list of basic blocks of x64 assembly code. In the case of the Var1
and Var2 rules, they are combined into a single basic block, \texttt{varBB},
which traverses the environment linked list until it reaches a closure, then
pushes an update marker to that location. The App rule is the simplest: it
consists of two instructions that push the location of the argument term along
with the current environment pointer onto the stack. The Lam and Update rules
are a little more complicated.  Recall that there is no rule in which we have a
lambda as our current term, and an empty stack, because that is the condition
for termination. Thus, we must add a check to see if the stack is empty
(\texttt{checkTermBB}), and terminate if it is (\texttt{termBB}). If it is not,
then we check (\texttt{checkUpdateBB}) to see if the code is NULL (update
marker) and if it is, update the marker and continue with the current closure.
If it is not an update marker, it is an argument closure, so we pop to extend
our current environment with it and continue into the body of the current
lambda.

The state of the machine in the implementation is defined by the heap, the
stack, and 4 registers: \texttt{rip} refers to the current term, \texttt{rax}
holds the current environment pointer, \texttt{rsp} holds the current stack
location, and \texttt{rbx} holds the next heap location. We also use
\texttt{rbp} to check for termination, and \texttt{rcx} for temporary storage.

\input{figures/impl}

\section{Discussion}
The $L$ machine has one primary strength: closure creation always takes 
constant time, a property we don't believe any other call-by-need implementation
can claim in general, due to construction of flat environments.

There is also a sense in which the $L$ machine is \emph{lazier} than flat
environment implementations. Instead of spending time allocating a flat
environment to optimize variable lookup (that may never get used), our
implementation does the minimal amount of preparation work. This approach is a
trade-off, as it comes with the previously mentioned cost of non-constant
variable lookup time. It seems likely that an ideal implementation will combine
the two techniques. 

There are also many known improvements that can be directly applied to the $L$
machine, as it is an implementation of a lazy Krivine machine. In particular,
Friedman et al.'s work on improving the lazy Krivine machine by collapsing
update markers (among other things) should be directly applicable \cite{lkm}.
Indeed, one can see the need for this in Figure~\ref{fig:states}, where multiple
update markers are pushed, and their locations are updated to the same value.

\section{Conclusion and Future Work}
We claim that this simple implementation has pedagogical value. The combination
of the simple heap structure with the straightforward instructions gives an
elegant and easy-to-understand mapping between call-by-need evaluation and
modern hardware.

Furthermore, we believe the $L$ machine could be a good target for abstraction
thanks to its simplicity, following the approach of Van Horn et
al \cite{van2010abstracting}. For example, its straightforward heap structure
could be amenable to abstraction, allowing for easier analysis of static heap
properties.

Beyond simplicity and elegance, however, we are actively exploring compile-time
optimizations to make this abstract machine practically usable for lazy language
implementation. Given a more efficient code generator, we believe it could be a
good target for parallel implementations due to its lightweight closures (every
closure consists of two machine words).

% We recommend abbrvnat bibliography style.
\bibliographystyle{abbrvnat}
\bibliography{annotated}

\end{document}

